\documentclass{article}
\input{shared_preamble.tex}
\usepackage{enumitem}
\newcommand{\utrain}{^{\textnormal{(train)}}}
\newcommand{\strain}{_{\textnormal{train}}}
\DeclareMathOperator*{\plim}{plim}

\title{Exercises for Chapter 5: Machine Learning Basics}
\begin{document}
\maketitle

\section*{Which exercise goes with which section?}
\begin{itemize}
    \item Exercise \ref{ML_ex_regression}: 5.1.4 Example: Linear Regression.
    \item Exercise \ref{ML_ex_hyperparameter}: 5.3 Hyperparameters and Validation Sets.
    \item Exercise \ref{ML_ex_bias}: 5.4.2 Bias.
    \item Exercise \ref{ML_ex_consistency}: 5.4.5 Consistency.
    \item Exercise \ref{ML_ex_maximumlikelihood}: 5.5 Maximum Likelihood Estimation.
    \item Exercise \ref{ML_ex_MAP}: 5.6.1 Maximum a Posteriori Estimation.
    \item Exercise \ref{ML_ex_kerneltrick}: 5.7.2 Support Vector Machines.
    \item Exercise \ref{ML_ex_SGD}: 5.9 Stochastic Gradient Descent.
\end{itemize}

\section*{Exercises}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \label{ML_ex_regression} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The linear regression example sets a certain gradient equal to zero to set up the \emph{normal equations} for a general training set $\mX\utrain$, weights $\vw$ and outputs $\vy\utrain$. This exercise works out a concrete example.

Suppose we are trying to discover the formula $y = 4x_1 - 2x_2$ by linear regression. Let
\begin{equation*}
  \mX\utrain =  \begin{pmatrix}
                1 & 0 \\
                0 & 4 \\
                2 & 5
                \end{pmatrix}, 
  \vy\utrain =  \begin{pmatrix}
                4 \\
                -8 \\
                -2
                \end{pmatrix},
  \vw           = \begin{pmatrix}
                w_1 \\
                w_2
                \end{pmatrix}.
\end{equation*}                
\begin{enumerate}
    \item Compute the $3\times1$ vector $\hat{\vy}\utrain = \mX\utrain\vw$. 
    \item Compute the scalar $MSE\strain = \frac13 \lVert \hat{\vy}\utrain - \vy\utrain \rVert^2$.
    \item Find $\frac{\partial}{\partial w_1} MSE\strain$ and $\frac{\partial}{\partial w_2} MSE\strain$. Put them together to form the $2\times1$ vector $\nabla_\vw MSE\strain$.
    \item The computation in the book shows that this vector equals zero exactly when ${\mX\utrain}^T\mX\utrain\vw - {\mX\utrain}^T\vy\utrain = 0$. Compute this second expression to see how the expressions are related.
    \item Solve the normal equations in the previous part to find $w_1$ and $w_2$. Are they correct?
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \label{ML_ex_hyperparameter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This exercise investigates the hyperparameter $\lambda$ discussed in the linear regression example. Suppose we are trying to fit a polynomial to the following data:
\begin{center}
\begin{tabular}{c|c}
    x & y \\
    \hline
    0 & 0 \\
    1 & 3 \\
    2 & 12
\end{tabular}
\end{center}
and we give our model the capacity to fit cubic data, i.e. our model is
\begin{equation*} y = w_0 + w_1 x + w_2 x^2 + w_3 x^3. \end{equation*}

\begin{enumerate}
    \item Show that our model has a bit too much capacity, because you could fit the data perfectly with a simple quadratic polynomial.
    \item Show we can overfit the data with $w_0 = 0, w_1 = 2, w_2 = 0, w_3 = 1$.
    \item Find the values of $MSE\strain$ for the quadratic model in (a) and the cubic model in (b).
    \item The best \emph{line} for fitting this data is $y = 6x - 1$. Find $MSE\strain$ for this model. 
    \item Now let $J(\vw) = MSE\strain + \lambda \vw^T\vw$ as in the book. Here $\lambda > 0$ is a \emph{hyperparameter}, a positive real number we have not decided yet. This imposes a cost for fitting the data with large coefficients. Find $J(\vw)$ for each of the three models above.
    \item This method never prefers the simple quadratic over the cubic, no matter what $\lambda$ is. Why not? 
    \item How could we fix $J(\vw)$ to prefer the model we consider to be simpler?
\end{enumerate}
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \label{ML_ex_bias} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We said that the sample variance is a \emph{biased estimator} of the variance of a Gaussian distribution, because $\E(\hat{\sigma}_m^2) = \frac{m-1}{m}\sigma^2$ instead of exactly $\sigma^2$.

\begin{enumerate}
    \item Compute $\lim_{m\to\infty} \E(\hat{\sigma}_m^2) - \sigma^2$.
    \item What does that tell us about the sample variance as an estimator?
\end{enumerate}
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \label{ML_ex_consistency} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This exercise investigates the \emph{(weak) consistency} of the sample mean $\hat{\mu}_m$ as an estimator of the mean of a normal distribution. In this exercise we will use the standard normal distribution with $\mu = 0$ and $\sigma = 1$. 

\begin{enumerate}
    \item Let $\eps = 2$ and $m = 1$. Find $P\left(\lvert \hat{\mu}_m - \mu \rvert > \eps \right)$. 
    \item The Central Limit Theorem says that for large $m$, the sample mean $\hat{\mu}_m$ is approximately normally distributed with mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{m}}$. For $\eps = 2$ and $m = 100$, find $P\left(\lvert \hat{\mu}_m - \mu \rvert > \eps \right)$.
    \item What happens to this probability as $m \to \infty$? \item Does it matter what $\eps$ is?
    \item Find $\plim_{m\to\infty} \hat{\mu}_m$. Conclude that the sample mean is a \emph{consistent} estimator for the mean of the standard normal distribution. 
\end{enumerate}
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \label{ML_ex_maximumlikelihood} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This exercise dives into the definitions of the \emph{maximum likelihood estimator} for $\vtheta$:
\begin{align}
    \vtheta_\textnormal{ML} & = \argmax_\vtheta \pmodel(\mathbb{X}; \vtheta)                  \nonumber    \\
                           & = \argmax_\vtheta \prod_{i = 1}^m \pmodel(\vx^{(i)}; \vtheta)   \label{TMLdef1} \\
                           & = \argmax_\vtheta \sum_{i = 1}^m \log \pmodel(\vx^{(i)}; \vtheta)   \label{TMLdef2}
\end{align}

In this exercise $x$ and $\theta$ will be whole numbers and we will use a probability distribution $P$ instead of a probability density function $p$.
\begin{enumerate}
    \item Consider the distribution $\Pmodel(-;3)$ given by 
    \begin{equation*}
    \Pmodel(x;3) = 
    \begin{cases}
        9\cdot10^{3-x} & \text{ for } x > 3 \\
        0              & \text{ for } x \leq 3
    \end{cases}
    \end{equation*}
    where $x$ is a whole number. Show that this a probability distribution.
    \item Fix a whole number $\theta > 0$. Consider the distribution $\Pmodel(-;\theta)$ given by 
    \begin{equation*}
    \Pmodel(x;\theta) = 
    \begin{cases}
        9\cdot10^{\theta-x} & \text{ for } x > \theta  \\
        0              & \text{ for } x \leq \theta
    \end{cases}
    \end{equation*}
    where $x$ is a whole number. Show that this a probability distribution.
    \item You have a single data point $x = 8$. Find $\Pmodel(8; \theta)$ and use it to find $\argmax_\theta \Pmodel(8; \theta)$.
    \item You have three data points $x^{(1)} = 7, x^{(2)} = 4, x^{(3)} = 5$. Together these three data points are referred to as $\mathbb{X}$. Find $\Pmodel(\mathbb{X}; 2)$, $\Pmodel(\mathbb{X}; 3)$, and $\Pmodel(\mathbb{X}; 4)$.
    \item Use definition \ref{TMLdef1} to find $\theta_\textnormal{ML}$ for this data set.
    \item Use definition \ref{TMLdef2} with a base-10 logarithm to find $\theta_\textnormal{ML}$ for this data set.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \label{ML_ex_MAP} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This exercise investigates the concept of MAP Estimation. Suppose we are trying to discover the linear relationship $y = 4x_1 - 2x_2$ by linear regression as in Exercise \ref{ML_ex_regression}. We model the relationship as $y = w_1x_1 + w_2x_2$ and use as our prior $p(\vtheta) = \gN(\vw; 0; \frac{1}{\lambda}\mI^2)$. 

\begin{enumerate}
    \item Write this probability distribution as a function $p(w_1,w_2)$.
    \item Compute $\log p(w_1,w_2)$.
    \item Show that the $\log$-prior term $\log p(w_1,w_2)$ is proportional to $\lambda \vw^T\vw$ plus a term that does not depend on $\vw$. A term that does not depend on $\vw$ does not affect the learning process.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \label{ML_ex_kerneltrick} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This exercise demonstrates a kernel trick. Suppose you are trying to use a Support Vector Machine to divide the $xy$-plane into two classes: the points inside the ellipse $x^2 + 9y^2 = 9$ and those outside the ellipse. This relationship is not linear, so it cannot be handled by an SVM.
\begin{enumerate} 
    \item Define a feature function $\phi$ by   
                    $\phi\left( \begin{smallmatrix} x \\ y           \end{smallmatrix} \right) 
                       = \left( \begin{smallmatrix} x^2 \\ xy \\ y^2 \end{smallmatrix} \right)$ 
    and let $\vx^{(1)} = \left( \begin{smallmatrix} 2 \\ 0           \end{smallmatrix} \right), 
             \vx^{(2)} = \left( \begin{smallmatrix} 0 \\ 2           \end{smallmatrix} \right)$. 
          Find $\phi\left(\vx^{(i)}\right)$ for each $i$.
    \item      Find a simplified expression for $\phi\left( \begin{smallmatrix} x \\ y \end{smallmatrix} \right)^T \cdot \phi\left(\vx^{(1)}\right)$ 
                           and another one for  $\phi\left( \begin{smallmatrix} x \\ y \end{smallmatrix} \right)^T \cdot \phi\left(\vx^{(2)}\right)$.
    \item Show that if we define the kernel $k$ by $k\left( 
                                                     \left( \begin{smallmatrix} x \\ y \end{smallmatrix} \right),
                                                     \left( \begin{smallmatrix} a \\ b \end{smallmatrix} \right) \right)
                            = a^2x^2 + b^2y^2$ then $k(\vx, \vx^{(i)}) = \phi\left(\vx\right)^T \cdot \phi\left(\vx^{(i)}\right)$ . Note that this allows us to get the benefit of the feature function $\phi$ without ever actually using the function $\phi$.
    \item Find $\alpha_1, \alpha_2, b$ so that if $\vx = \left( \begin{smallmatrix} x \\ y \end{smallmatrix} \right)$, 
          then $f(\vx) = b + \sum_i \alpha_i k( \vx , \vx^{(i)})$ is positive when $x^2 + 9y^2 > 9$ and negative when  $x^2 + 9y^2 < 9$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \label{ML_ex_SGD} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This exercise illustrates Stochastic Gradient Descent on a linear regression problem. Consider the data set 
\begin{tabular}{c|c}
    x & y \\
    \hline
    1 & 1 \\
    1 & 3 
\end{tabular}
and the attempt to fit a line $y = \theta x$ to it. To this end we will let 
$$p(y|x;\theta) = \mathcal{N}(y;\theta x, 1) = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac12(y-\theta x)^2\right).$$
\begin{enumerate}
    \item Find $J(\theta) = \frac12 \sum_{i=1}^2 -\log p(y^{(i)} | x^{(i)}; \theta)$ in this situation.
    \item Find $g = \nabla_\theta J(\theta)$.
    \item Suppose your initial estimate for $\theta$ was $\theta = 4$. Use the algorithm $\theta \leftarrow \theta - \eps g$ and investigate what happens if $\eps = 1$, $\eps = 2$, and $\eps = 0.5$.
    \item Classify all values of $\epsilon$ based on the qualitative behavior of the Gradient Descent algorithm for this example. 
    \item The previous parts illustrated Gradient Descent. To investigate Stochastic Gradient Descent, instead of having two data points, imagine the data set has untold millions of data points, half of which have $y = 3x$ and half of which have $y = x$. At each step, we will take a random sample of $m = 2$ of these points and apply gradient descent. Investigate what happens with $\eps = 1$ in this situation.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{\em Exercises contributed by Chris Cunningham}
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Solutions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \label{ML_ex_regression_solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
    \item $\hat{\vy}\utrain = \begin{pmatrix} w_1\\ 4w_2 \\ 2w_1 + 5w_2 \end{pmatrix}$
    \item $MSE\strain = \frac13\left(\left(w_1-4\right)^2 + \left(4w_2+8\right)^2+\left(2w_1+5w_2+2\right)^2\right)$ 
    \item Compute the derivatives using the chain rule instead of squaring those polynomials:
    \begin{align*} \nabla_\vw MSE\strain & = \phantom{\frac13} \begin{pmatrix}
                                            \frac{\partial}{\partial w_1}MSE\strain   \\ 
                                            \frac{\partial}{\partial w_2}MSE\strain
                                            \end{pmatrix} \\
                              & = \frac13 \begin{pmatrix}
                                            2(w_1-4)        & + 2(2w_1+5w_2+2)\cdot 2 \\ 
                                            2(4w_2+8)\cdot4 & + 2(2w_1+5w_2+2)\cdot 5
                                            \end{pmatrix} \\
                              & = \frac23 \begin{pmatrix}
                                            \phantom2(w_1-4)         & + \phantom2(2w_1+5w_2+2)\cdot 2  \\ 
                                            \phantom2(4w_2+8)\cdot4  & + \phantom2(2w_1+5w_2+2)\cdot 5
                                            \end{pmatrix} \\
                              & = \frac23 \begin{pmatrix}
                                            5w_1 + 10w_2 \\ 
                                            10w_1 + 41w_2 + 42                       
                                            \end{pmatrix}
     \end{align*}
    \item Multiply those matrices: 
        \begin{align*}
            {\mX\utrain}^T\mX\utrain\vw & = 
            \begin{pmatrix}1&0&2\\0&4&5\end{pmatrix}
            \begin{pmatrix}1&0\\0&4\\2&5\end{pmatrix}
            \begin{pmatrix}w_1\\w_2\end{pmatrix} \\
                                      & = 
            \begin{pmatrix}5&10\\10&41\end{pmatrix}
            \begin{pmatrix}w_1\\w_2\end{pmatrix} \\
                                      & = 
             \begin{pmatrix}5w_1+10w_2\\10w_1+41w_2\end{pmatrix}. \\
            {\mX\utrain}^T\vy\utrain    & =
            \begin{pmatrix}1&0&2\\0&4&5\end{pmatrix}
            \begin{pmatrix}4\\-8\\-2\end{pmatrix} \\
                                      & = 
            \begin{pmatrix}0\\-42\end{pmatrix}. \\
        \end{align*}
        So ${\mX\utrain}^T\mX\utrain\vw - {\mX\utrain}^T\vy\utrain =
        \begin{pmatrix}
            5w_1 + 10w_2 \\ 
            10w_1 + 41w_2 + 42
            \end{pmatrix}$, the same matrix that appeared in the previous part.
    \item If $5w_1 + 10w_2 = 0$ then $w_1 = -2w_2$. Substitute into the second equation to get $10\cdot-2w_2 + 41w_2 + 42 = 0$; $21w_2 = -42$, so $w_2 = -2$ and $w_1 = 4$.
    
    This is just as we intended at the start of the problem: The solution is $y = \vw^T\vx = 4x_1 - 2x_2$, which perfectly fits all three data points.
\end{enumerate}
    

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \label{ML_ex_hyperparameter_solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
    \item Each of these values of $y$ satisfies $y = 3x^2$.
    \item Each of these values of $y$ also satisfies $y = x^3 + 2x$.
    \item With a perfect fit, $MSE\strain = 0$ for both models.
    \item Here the model predicts $y = -1, 5, 11$ when the true values are $y = 0, 3, 12$. The sum of the squares of the errors are $1 + 4 + 1 = 6$, so the mean squared error is $2$.
    \item Here, $\vw^T\vw$ is the sum of the squares of the coefficients of the polynomial. For the cubic, that is 9. For the quadratic, that is 5. For the line, that is 37. So for the three cases,
    \begin{tabular}{l|c}
    Model & $J(\vw)$ \\
    \hline
    $y = 3x^2$     & $9\lambda$ \\
    $y = x^3 + 2x$ & $5\lambda$ \\
    $y = 6x - 1$   & $2 + 37\lambda$
    \end{tabular}
    \item $9\lambda \geq 5\lambda$ for any positive value of $\lambda$.
    \item This function $J$ only imposes a cost for using large coefficients, no matter which coefficients are nonzero. If we think it is ``simpler'' to use a quadratic than a cubic, then we need to build that into our function $J$. Above, we used 
    \begin{equation*} J(\vw) = MSE\strain + \lambda w_0^2+\lambda w_1^2+\lambda w_2^2+\lambda w_3^2. \end{equation*}
    We could instead use something like
    \begin{equation*} J(\vw) = MSE\strain + \lambda w_0^2+\lambda w_1^2+ \lambda^2 w_2^2+ \lambda^3 w_3^2 \end{equation*}
    if we wanted to penalize the model for using the higher powers of $x$. For the examples above, we would then end up with

    \begin{tabular}{l|c}
    Model & $J(\vw)$ \\
    \hline
    $y = 3x^2$     & $9\lambda^2$ \\
    $y = x^3 + 2x$ & $4\lambda + \lambda^3$ \\
    $y = 6x - 1$   & $2 + 37\lambda$
    \end{tabular}.
    
    Then, for large values of $\lambda$, we will prefer the underfit model $y = 6x - 1$. For $\lambda = 0$, we cannot distinguish between the quadratic and the cubic. For intermediate values of $\lambda$ like $\lambda = \frac13$, we will prefer the ``simpler'' quadratic as desired. 
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \label{ML_ex_bias_solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
    \item $\lim_{m\to\infty} \E(\hat{\sigma}_m^2) - \sigma^2 = \lim_{m\to\infty} \frac{m-1}{m}\sigma^2 - \sigma^2 = \sigma^2 - \sigma^2 = 0$.
    \item The sample variance is an \emph{asymptotically unbiased estimator}.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \label{ML_ex_consistency_solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate} 
    \item When $m=1$, $\hat{\mu}_m$ is just a single data point. In this distribution, the probability that a single data point is more than two standard deviations away from the mean is $P(|z| > 2)$ which is about $1 -0.95 = 0.05$.
    \item According to the assumptions we have made, $\lvert \hat{\mu}_m - \mu \rvert$ is normally distributed with a mean of 0 and a standard deviation of $\frac{1}{10}$. The probability that this quantity is larger than 2 is $P(|z| > 20)$ which is about $5.5 \times 10^{-89}$.
    \item For large $m$, $P\left(\lvert \hat{\mu}_m - \mu \rvert > \eps \right) = P(|z| > 2 \sqrt{m})$, which approaches zero as $m \to \infty$.
    \item No, for any fixed $\eps$ and large $m$ this probability is $P(|z| > \eps\sqrt{m})$, which approaches zero as $m \to \infty$ for any fixed $\eps$.
    \item We argued that for any $\eps > 0$, $P(\lvert \hat{\mu}_m - \mu \rvert > \eps) \to 0$ as $m \to \infty$. This means that $\plim_{m \to \infty} \hat{\mu}_m = \mu$. This shows that the sample mean is a (weakly) consistent estimator for the mean of the standard normal distribution.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \label{ML_ex_maximumlikelihood_solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
    \item Each probability is between 0 and 1, and the sum of the probabilities is $0 + 0 + 0 + 0 + 0.9 + 0.09 + 0.009 + \cdots = 0.\overline{9} = 1$.
    \item The probabilities are all 0 when $x \leq \theta$, but the next one is $0.9$, and the total is again $0.9 + 0.09 + 0.009 +\cdots = 0.\overline{9} = 1$.
    \item When $\theta \geq 8$, $\Pmodel(8; \theta) = 0$. When $\theta < 8$, $\Pmodel(8; \theta) = 9\cdot10^{\theta - 8}$. This is maximized when $\theta = 7$, when the probability is $0.9$.
    \item If $\theta = 2$, then the probability $\Pmodel(7; 2)$ of getting $x = 7$ would be $9 \cdot 10^{2-7} = 0.00009$. We can do this for each $\theta$ and each $x$:
        
    $\begin{array}{c|c|c|c||c}
        \theta & \Pmodel(7; \theta) & \Pmodel(4; \theta) & \Pmodel(5; \theta) & \Pmodel(\mathbb{X}; \theta) = \prod \Pmodel(x;\theta) \\
        \hline
        2 & 9 \cdot 10^{-5} & 9 \cdot 10^{-2} & 9 \cdot 10^{-3} & 9^3 \cdot 10^{-10}\\
        3 & 9 \cdot 10^{-4} & 9 \cdot 10^{-1} & 9 \cdot 10^{-2} & 9^3 \cdot 10^{-7}\\
        4 & 9 \cdot 10^{-3} & 0 & 9 \cdot 10^{-1} & 0\\
    \end{array}$
    \item The product of the probabilities is maximized when $\theta = 3$, so we conclude that $\theta_\textnormal{ML} = 3$. 
    \item When we take (base 10) logarithms and add the probabilities instead of multiplying the probabilities, the numbers stay in the same order:
    $\begin{array}{c|c|c|c||c}
        \theta & \log\Pmodel(7; \theta) & \log\Pmodel(4; \theta) & \log\Pmodel(5; \theta) & \vbox{\hbox{\strut$\log\Pmodel(\mathbb{X}; \theta)$}\hbox{\strut $= \sum \log\Pmodel(x;\theta)$}} \\
        \hline
        2 & -5 + \log 9 & -2 + \log 9 & -3 + \log 9 & -10 + 3 \log 9 \\
        3 & -4 + \log 9 & -1 + \log 9 & -2 + \log 9 & -7  + 3 \log 9 \\
        4 & -3 + \log 9 & -\infty     & -1 + \log 9 & -\infty \\
    \end{array}$
    This is still maximized at $\theta = 3$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \label{ML_ex_MAP_solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
    \item In this case the prior is 
$$p(w_1, w_2) = \frac{\lambda}{2\pi}\exp\left(-\frac\lambda2(w_1^2+w_2^2)\right).$$
    \item If $p(w_1, w_2) = \frac{\lambda}{2\pi}\exp\left(-\frac\lambda2(w_1^2+w_2^2)\right)$, then using the natural logarithm, $\log p(w_1,w_2) = - \frac{\lambda}{2}(w_1^2+w_2^2) + \log \frac{\lambda}{2\pi}$. 
    \item $\lambda \vw^T \vw = \lambda(w_1^2 + w_2^2)$, so indeed the $\log$-prior term is $-\frac12$ times $\lambda \vw^T \vw$, plus $\log\frac\lambda{2\pi}$ which does not affect the learning process.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \label{ML_ex_kerneltrick_solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
    \item $\phi\left( \vx^{(1)}\right) = \left( \begin{smallmatrix} 4 \\ 0 \\ 0 \end{smallmatrix} \right);
           \phi\left( \vx^{(2)}\right) = \left( \begin{smallmatrix} 0 \\ 0 \\ 4 \end{smallmatrix} \right)$.
    \item $\phi\left( \begin{smallmatrix} x \\ y         \end{smallmatrix} \right)^T \cdot \phi\left(\vx^{(1)}\right) = 
               \left( \begin{smallmatrix} x^2 & xy & y^2 \end{smallmatrix} \right)   \cdot \left( \begin{smallmatrix} 4 \\ 0 \\ 0 \end{smallmatrix} \right) = 4x^2.$
    \newline
          $\phi\left( \begin{smallmatrix} x \\ y         \end{smallmatrix} \right)^T \cdot \phi\left(\vx^{(2)}\right) = 
               \left( \begin{smallmatrix} x^2 & xy & y^2 \end{smallmatrix}\right)    \cdot \left( \begin{smallmatrix} 0 \\ 0 \\ 4 \end{smallmatrix} \right) = 4y^2.$
    \item For $i=1$ we have $a=2, b=0$ so $k\left(\vx, \vx^{(1)}\right) = 4x^2$ as desired. 
    \newline
    For $i=2$ we have $a=0, b=2$ so $k\left(\vx, \vx^{(2)}\right) = 4y^2$ as desired.
    \item There are many correct answers; one is that $x^2 + 9y^2 = 9$ can be written as $4x^2 + 36y^2 = 36$, so we can use $b = -36, \alpha_1 = 1, \alpha_2 = 9$. Then $f(\vx) = -36 + 1\cdot4x^2 + 9\cdot 4y^2$ is positive when $x^2 + 9y^2 > 9$ and negative when $x^2 + 9y^2 < 9$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \label{ML_ex_SGD_solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\MLexSGDlog}{\log\left(\frac{1}{\sqrt{2\pi}}\right)}
\begin{enumerate} 
    \item
    \begin{align*}
        p(y|x; \theta)                  & = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac12(y-\theta x)^2\right) \\
        -\log p(y|x;\theta)             & = -\MLexSGDlog + \frac12(y-\theta x)^2.  \\
        \text{Now plug in the data points:}\\
        -\log p(y^{(1)}|x^{(1)};\theta) & = -\MLexSGDlog + \frac12(1-\theta)^2  \\
        -\log p(y^{(2)}|x^{(2)};\theta) & = -\MLexSGDlog + \frac12(3-\theta)^2  \\
        \text{Add and divide by 2 to find } J(\theta): \\
    \sum-\log p(y^{(i)}|x^{(i)};\theta) & = -2\MLexSGDlog + \frac12(1-\theta)^2+\frac12(3-\theta)^2 \\
        J(\theta)                      & = -\phantom{2}\MLexSGDlog + \frac14(1-\theta)^2 + \frac14(3-\theta)^2
    \end{align*}
    \item $g = \nabla_\theta J(\theta) = -\frac12(1-\theta) - \frac12(3-\theta) = \theta - 2$.
    \item Gradient Descent will replace $\theta$ at each step with $\theta - \eps g = \theta - \eps(\theta-2)$. 
    
    \underline{For $\eps = 1$} this replaces $\theta$ with $2$ immediately, regardless of where we start.
    
    \underline{For $\eps = 2$}, at each step, $\theta$ is replaced with $\theta - 2(\theta-2) = 4 - \theta$. Starting at $\theta = 4$, we would then go to $\theta = 0$, then back to $\theta = 4$, and we would never converge on the optimum $\theta = 2$.
    
    \underline{For $\eps = 0.5$}, at each step, $\theta$ is replaced by $\theta - \frac12(\theta - 2) = \frac12\theta + 1$. So starting at $\theta = 4$, the sequence of parameters is $4, 3, 2.5, 2.25, 2.125, \ldots$, slowly converging to the optimum $\theta = 2$. 
    \item For $\eps > 2$, the values of $\theta$ grow without bound. For $1 < \eps < 2$, the values of $\theta$ overshoot the optimum at each step but still converge. At $\eps = 1$, we hit the optimum in one step. For $0 < \eps < 1$, the values of $\theta$ converge to the optimum.
    \item Imagine a random data point $(x^{(i)}, y^{(i)})$ which has equal chance to satisfy $y = 3x$ or $y = x$. To summarize this, we can write that $y = (2\pm1)x$. Then its contribution to $J(\theta)$ is $\MLexSGDlog$ plus 
    \begin{equation*}
        \frac12\left(y^{(i)}-\theta x^{(i)}\right)^2 = \frac12\left((2\pm 1)x^{(i)}-\theta x^{(i)}\right)^2
    \end{equation*}
    and its contribution to $g = \nabla_\theta J(\theta)$ is
    \begin{align*}
          & -\frac12 x^{(i)}     \left((2 \pm 1)x^{(i)} -\theta x^{(i)}\right) \\
        = & -\frac12 {x^{(i)}}^2 \left( 2 \pm 1 - \theta \right)
    \end{align*}
    
    Now assuming $\eps = 1$, the ideal situation would be when $x^{(1)} = x^{(2)} = 1$ and we get one of each type of data point. In that situation we immediately head to $\theta = 2$ as in the previous steps. The worst-case situations come when the $|x^{(i)}|$ are large, because that puts us in the situation from the previous parts where the values of $\theta$ grow without bound. A lower value of $\eps$ based on the maximum size of $|x^{(i)}|$ would counteract this effect.
    
    Of course sometimes both of the $\pm$ will come out in the same direction, which will push $\theta$ toward $\theta = 3$ or toward $\theta = 1$. We could fix this easily with a larger value of $m$ instead of $m = 2$. 

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{\em Solutions contributed by Chris Cunningham}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
